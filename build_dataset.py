# encoding: utf-8
"""
creates or transforms the dataset e.g. sentences.txt, labels.txt
"""

import argparse
import csv
import re

import jieba

from model.helper import flatten

# import unicodedata


VOCAB_PATH = "./chinese_vectors/sgns.weibo.bigram-char"

parser = argparse.ArgumentParser()

parser.add_argument('--model_dir', default='experiments/base_model',
                    help="Directory containing params.json")
parser.add_argument('--data_dir', default='data/small',
                    help="Directory containing the dataset")
parser.add_argument('--restore_dir', default=None,
                    help="Optional, directory containing weights to reload before training")


test_line = "ğŸ¤” åˆæ˜¯ä¸€å®¶èœå•è®©äººçœ¼èŠ±ç¼­ä¹±çš„åº—ï¼è”¡å¡˜çš„è´ç¢Ÿè½©ä¸åœ¨å¹¿åœºç¾é£ŸåŒºå†…ï¼Œè€Œæ˜¯åœ¨ä¸‡ä½³é…’åº—çš„å¤§å…æ—è¾¹ï¼Œé‡Œé¢æ›²æ›²æŠ˜æŠ˜çš„åˆ†æˆå¾ˆå¤šç”¨é¤åŒºï¼Œä½†å…¶å®æ¡Œæ•°ä¸å°‘ï¼Œç”¨é¤åŒºéš”å¾—æ¯”è¾ƒæœ‰ç§å¯†æ„Ÿï¼Œé€‚åˆå°èšï¼Œè£…æ½¢è·Ÿå®ƒçš„èœè‰²ä¸€æ ·èµ°æ··æ­é£ï¼\n\nè´ç¢Ÿè½©ä¹Ÿæ˜¯åŒ…å±±åŒ…æµ·ï¼Œå£å‘³å°±ä¸€èˆ¬ä»¥ä¸Šï¼Œè¾¾äººæœªæ»¡ï¼Œå¦‚æœäº¤é€šæ–¹ä¾¿åœ¨é™„è¿‘å¯ä»¥è€ƒè™‘ï¼Œä½†ä¸å¿…è¦ç‰¹åœ°æ¥åƒï¼Œèœçš„ä»½é‡å¾ˆåˆšå¥½ï¼Œ2äººä¹Ÿå¯ä»¥ç‚¹å‡ºèŠ±æ ·æ¥ã€‚\n\nğŸ“æ³•å›½é¹…è‚ç‚’é¥­ï¼šé¹…è‚å‘³é“å¾ˆæ·¡ä»½é‡ä¸å¤šï¼Œæ‰€ä»¥åº”è¯¥æŠŠé‡ç‚¹æ”¾åœ¨é¥­ç‚’å¾—æœ‰å¤šé¦™ï¼Œä¸€èˆ¬èˆ¬åƒå¦ˆå¦ˆèœï¼Œé’Ÿæ„ç‰›æ’çš„å¥½åƒå¾ˆå¤šã€‚\n\nğŸ™ç†Ÿæ‚‰çš„å¢¨é±¼ä»”ï¼šæ³°å¼å‡‰æ‹Œæµ·é²œå£å‘³ï¼Œok!\n\nğŸ—ç”ŸèœåŒ…å…»çš„è‚‰ç¢ï¼šè‚‰æœ«ç‚’å››å­£è±†ï¼Œæ¯”è¾ƒå’¸ä¸€ç‚¹ï¼ŒåŒ…åœ¨ç”Ÿèœä¸­åˆšå¥½ï¼Œæœ‰è¾£åº¦ï¼\n\nğŸŒ™æœˆäº®è™¾è–„é¥¼ï¼šNG! è™¾åœ¨å“ªå‘¢ï¼Ÿæœˆäº®ä¸Šå—ï¼Ÿ è¯·æ›´åç‚¸è–„é¥¼ï¼Œä½†ä¹Ÿç‚¸å¾—ä¸å¥½ï¼Œæ²¹å“ˆå‘³å¾ˆé‡ã€‚è¿™45å¤§æ´‹æˆ‘è§‰å¾—å¤ªè´µï¼è®“"

# åˆ¤æ–­ä¸­æ–‡èŒƒå›´
# ! https://blog.csdn.net/JohinieLi/article/details/76152549


def get_vocab(file_path):
    word_set = set()
    with open(file_path, newline='', encoding='utf-8', errors='ignore') as f:
        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)
        next(reader)    # è·³è¿‡ç¬¬ä¸€è¡Œ
        for word, *_ in reader:
            word_set.add(word)
    return word_set


def _full_cut(word, vocab):
    res = []
    tmp = jieba.lcut(word, cut_all=True)
    for i in (0, -1):
        if tmp[i] in vocab:
            res.append(tmp[i])
    return res if len(res) > 0 else "<UNK>"  # å°†vocabé‡Œæœªå‡ºç°çš„wordæ›¿æ¢ä¸º<UNK>


def tokenize_word(line, vocab):
    rule = re.compile("[^a-zA-Z0-9\u4e00-\u9fa5]")
    line = rule.sub('', line)

    sentence = []
    for word in jieba.cut(line, cut_all=False):
        if word in vocab:
            try:
                int(word)
                sentence.append("<NUM>")  # å°†æ•°å­— -> <NUM>
            except ValueError:
                sentence.append(word)
        else:
            sentence.append(_full_cut(word, vocab))
    return list(flatten(sentence))


def load_dataset(path_csv, test=False):
    # with open(path_csv) as in_file:
    #     next(in_file)
    #     for idx, line in enumerate(in_file):
    #         if idx < 3:
    #             print(line)
    #         else:
                # break
    with open(path_csv, newline='', encoding='utf-8', errors='ignore') as in_file:
        reader = csv.reader(in_file, delimiter=',')
        _, _, *label_headers = next(reader)
        for idx, sentence, *labels in reader:
            if int(idx) < 20:
                print(_remove_punctuation(sentence))
            else:
                break


# load_dataset("./data/train/sentiment_analysis_trainingset.csv")


def main():
    args = parser.parse_args()
    vocab = get_vocab(VOCAB_PATH)
    print(tokenize_word(test_line, vocab))

    # print(len(vocab))


if __name__ == "__main__":
    main()
